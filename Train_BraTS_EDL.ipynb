{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vISEqEyRts-x",
        "outputId": "9d800033-643e-4d3e-ab5c-8d257a90d243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC 1: K·∫æT N·ªêI GOOGLE DRIVE\n",
        "# ----------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nOpH9fekWDg",
        "outputId": "45597c6c-5e46-4a09-e962-45cb4919780d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2GgoI-UuDVt",
        "outputId": "721cf6e9-8a65-4083-f0ae-54be94f3034c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PGo-Z2WGvpDZ"
      },
      "outputs": [],
      "source": [
        "# !ls nnUNet_Workspace/nnUNet/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "roUHjiNQxSJ1"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC 2: C√ÄI ƒê·∫∂T M√îI TR∆Ø·ªúNG & GI·∫¢I N√âN CODE\n",
        "# ----------------------------------------------------------------------\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jXnHExk4t40I",
        "outputId": "14f2a87c-7dd9-451d-dba9-790bc9715928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnUNet_Workspace\n",
            "/content/nnUNet_Workspace/nnUNet\n",
            "Obtaining file:///content/nnUNet_Workspace/nnUNet\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (2.9.0+cu126)\n",
            "Collecting acvl-utils<0.3,>=0.2.3 (from nnunetv2==2.6.2)\n",
            "  Downloading acvl_utils-0.2.5.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dynamic-network-architectures<0.5,>=0.4.1 (from nnunetv2==2.6.2)\n",
            "  Downloading dynamic_network_architectures-0.4.2.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (1.16.3)\n",
            "Collecting batchgenerators>=0.25.1 (from nnunetv2==2.6.2)\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (0.25.2)\n",
            "Collecting SimpleITK>=2.2.1 (from nnunetv2==2.6.2)\n",
            "  Downloading simpleitk-2.5.3-cp311-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (2.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (0.21)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (2025.10.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (2.32.4)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (5.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (0.13.2)\n",
            "Collecting imagecodecs (from nnunetv2==2.6.2)\n",
            "  Downloading imagecodecs-2025.11.11-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Collecting yacs (from nnunetv2==2.6.2)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting batchgeneratorsv2>=0.3.0 (from nnunetv2==2.6.2)\n",
            "  Downloading batchgeneratorsv2-0.3.0.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (0.8.1)\n",
            "Requirement already satisfied: blosc2>=3.0.0b1 in /usr/local/lib/python3.12/dist-packages (from nnunetv2==2.6.2) (3.11.1)\n",
            "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2.3->nnunetv2==2.6.2)\n",
            "  Downloading connected_components_3d-3.26.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.25.1->nnunetv2==2.6.2) (11.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.25.1->nnunetv2==2.6.2) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators>=0.25.1->nnunetv2==2.6.2)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.25.1->nnunetv2==2.6.2) (3.6.0)\n",
            "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.3.0->nnunetv2==2.6.2)\n",
            "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.12/dist-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (1.10.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (1.1.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (4.5.0)\n",
            "Requirement already satisfied: numexpr>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (2.14.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (9.0.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (1.0.22)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (2.37.2)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->nnunetv2==2.6.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->nnunetv2==2.6.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->nnunetv2==2.6.2) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->nnunetv2==2.6.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->nnunetv2==2.6.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->nnunetv2==2.6.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->nnunetv2==2.6.2) (2025.11.12)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->nnunetv2==2.6.2) (1.5.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from yacs->nnunetv2==2.6.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.6.2) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2==2.6.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.2->nnunetv2==2.6.2) (3.0.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.24.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.7.0)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2==2.6.2)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.25.1->nnunetv2==2.6.2)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (1.2.0)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2==2.6.2)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading simpleitk-2.5.3-cp311-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (52.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagecodecs-2025.11.11-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading connected_components_3d-3.26.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
            "  Building editable for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunetv2: filename=nnunetv2-2.6.2-0.editable-py3-none-any.whl size=16865 sha256=7a04d3327612245274e10f62659d51b36b867766756f718f7cd813f8cbda8719\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-59kqpvfh/wheels/30/76/34/cc12ee383702b9c8a15e0a83ed78089e7cdb1f7ddb59b264a4\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27213 sha256=e6f09039b370fd43d69cd83257f4027256a22d01dc473aa7f89d1af92368339d\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/c4/16/bae888b32a033f634d91a14256a8f8c8ec97db4e4dad8f0216\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=9ce599f76b0339bbb1058e712647a75f7c54ed4de0f2e81fbaeda6bafc6a089d\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/21/2b/7b25080f9f5847e6c3162b89d859d7cec9f3093158e56bd008\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.3.0-py3-none-any.whl size=65215 sha256=78d919c0e36bc8f757f1540ce24ea3a628b4ba5a68fa927d66d48c91e2b50e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/99/a0/1224a58f286be9fedbdb256d41e782a63b5ecab743e92ed302\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.4.2-py3-none-any.whl size=39025 sha256=4818bd029e4146c76306b253c933e04a8e4c5a9bb788de71097731bfa7b92c68\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/a8/c5/241ab34db40b3a6f498d2794411cc08a6309fefdc0d6e3d9f3\n",
            "Successfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
            "Installing collected packages: SimpleITK, linecache2, argparse, yacs, traceback2, imagecodecs, connected-components-3d, unittest2, batchgenerators, fft-conv-pytorch, acvl-utils, batchgeneratorsv2, dynamic-network-architectures, nnunetv2\n",
            "Successfully installed SimpleITK-2.5.3 acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.3.0 connected-components-3d-3.26.1 dynamic-network-architectures-0.4.2 fft-conv-pytorch-1.2.0 imagecodecs-2025.11.11 linecache2-1.0.0 nnunetv2-2.6.2 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "b9cb9c9b586b4c64bdef412aaa963bb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hiddenlayer\n",
            "  Downloading hiddenlayer-0.3-py3-none-any.whl.metadata (703 bytes)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (0.21)\n",
            "Downloading hiddenlayer-0.3-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2442155567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nnUNet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -e .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade hiddenlayer graphviz # C√†i th√™m visualizer n·∫øu c·∫ßn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             self._str = self._format_parsed_parts(self.drive, self.root,\n\u001b[0m\u001b[1;32m    444\u001b[0m                                                   self._tail) or '.'\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mroot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;34m\"\"\"The root of the path, if any.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# T·∫°o th∆∞ m·ª•c l√†m vi·ªác\n",
        "!mkdir -p /content/nnUNet_Workspace\n",
        "%cd /content/nnUNet_Workspace\n",
        "\n",
        "# Copy v√† gi·∫£i n√©n Code (Gi·∫£ s·ª≠ b·∫°n ƒë·ªÉ file zip ·ªü th∆∞ m·ª•c g·ªëc Drive/BraTS2020_EDL_Project)\n",
        "!cp \"/content/drive/MyDrive/XUM_project/nnUNet.zip\" .\n",
        "!unzip -q nnUNet.zip\n",
        "!rm nnUNet.zip\n",
        "\n",
        "# C√†i ƒë·∫∑t th∆∞ vi·ªán t·ª´ source code v·ª´a gi·∫£i n√©n\n",
        "%cd nnUNet\n",
        "!pip install -e .\n",
        "!pip install --upgrade hiddenlayer graphviz # C√†i th√™m visualizer n·∫øu c·∫ßn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DwDoA5Aum_ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be157ae5-7b4f-4ca1-dabb-18765daa3e6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  nnUNet_Workspace  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gi·∫£i n√©n `nnUNet_preprocessed` v√†o th∆∞ m·ª•c content c·ªßa Colab"
      ],
      "metadata": {
        "id": "uzdMKe6scnuH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1UiTXFMvDxV",
        "outputId": "96667df8-8c3f-4b93-83c8-474a56f68b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Copy file t·ª´ Drive‚Ä¶\n",
            "üí® Copy xong. B·∫Øt ƒë·∫ßu gi·∫£i n√©n‚Ä¶\n",
            "üîß Ph√°t hi·ªán folder root trong ZIP: nnUNET_preprocessed\n",
            "üîß ƒêang gi·∫£i n√©n, lo·∫°i b·ªè l·ªõp folder ƒë·∫ßu ti√™n‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üîÑ Unzipping: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2584/2584 [03:09<00:00, 13.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ GI·∫¢I N√âN XONG V√Ä ƒê√É T·ª∞ ƒê·ªòNG FIX FOLDER DOUBLE-LAYER! üòéüî•\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC 3: GI·∫¢I N√âN D·ªÆ LI·ªÜU PREPROCESSED ‚Äì AUTO FIX FOLDER DOUBLE-LAYER\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "os.environ['nnUNet_raw'] = \"/content/nnUNet_raw\"\n",
        "os.environ['nnUNet_preprocessed'] = \"/content/nnUNet_preprocessed\"\n",
        "os.environ['nnUNet_results'] = \"/content/drive/MyDrive/XUM_project/nnUNet_results\"\n",
        "\n",
        "!mkdir -p $nnUNet_raw\n",
        "!mkdir -p $nnUNet_preprocessed\n",
        "\n",
        "zip_path_drive = \"/content/drive/MyDrive/XUM_project/nnUNET_preprocessed.zip\"\n",
        "zip_local = \"/content/nnUNET_preprocessed.zip\"\n",
        "extract_root = \"/content/nnUNet_preprocessed\"\n",
        "\n",
        "print(\"üì¶ Copy file t·ª´ Drive‚Ä¶\")\n",
        "!cp \"$zip_path_drive\" \"$zip_local\"\n",
        "\n",
        "print(\"üí® Copy xong. B·∫Øt ƒë·∫ßu gi·∫£i n√©n‚Ä¶\")\n",
        "\n",
        "with zipfile.ZipFile(zip_local, 'r') as zip_ref:\n",
        "    # Detect folder root name inside zip (vd: nnUNET_preprocessed/)\n",
        "    root_name = zip_ref.namelist()[0].split('/')[0]\n",
        "\n",
        "    file_list = zip_ref.infolist()\n",
        "    total_files = len(file_list)\n",
        "\n",
        "    print(f\"üîß Ph√°t hi·ªán folder root trong ZIP: {root_name}\")\n",
        "    print(\"üîß ƒêang gi·∫£i n√©n, lo·∫°i b·ªè l·ªõp folder ƒë·∫ßu ti√™n‚Ä¶\")\n",
        "\n",
        "    for file in tqdm(file_list, total=total_files, desc=\"üîÑ Unzipping\"):\n",
        "        file_path = file.filename\n",
        "\n",
        "        # B·ªè l·ªõp folder ƒë·∫ßu ti√™n\n",
        "        fixed_path = file_path.replace(root_name + \"/\", \"\", 1)\n",
        "\n",
        "        if fixed_path.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        # Ch·ªâ ƒë·ªãnh n∆°i l∆∞u ƒë√∫ng\n",
        "        target_path = os.path.join(extract_root, fixed_path)\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c n·∫øu c·∫ßn\n",
        "        if file.is_dir():\n",
        "            os.makedirs(target_path, exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "            with zip_ref.open(file) as source, open(target_path, \"wb\") as target:\n",
        "                target.write(source.read())\n",
        "\n",
        "os.remove(zip_local)\n",
        "\n",
        "print(\"üéâ GI·∫¢I N√âN XONG V√Ä ƒê√É T·ª∞ ƒê·ªòNG FIX FOLDER DOUBLE-LAYER! üòéüî•\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gi·∫£i n√©n `nnUNet_raw` v√†o th∆∞ m·ª•c content c·ªßa Colab"
      ],
      "metadata": {
        "id": "3ddunCdkcuTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/XUM_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ1JgdTkfpsv",
        "outputId": "cb98039e-0998-4a38-c2a1-736e4f7abb8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inference_results\t nnUNET_raw.zip  run_batch_inference.py\n",
            "inference_results_batch  nnUNet_results  Train_BraTS_EDL.ipynb\n",
            "nnUNET_preprocessed.zip  nnUNet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Exists? ->\", os.path.exists(\"/content/drive/MyDrive/XUM_project/nnUNET_raw.zip\"))\n",
        "print(os.listdir(\"/content/drive/MyDrive/XUM_project\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSghFRLof6Dt",
        "outputId": "19e2a6d0-985d-4725-9dc2-22e78d0f29ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists? -> True\n",
            "['nnUNET_preprocessed.zip', 'nnUNET_raw.zip', 'nnUNet_results', 'nnUNet.zip', 'inference_results', '.ipynb_checkpoints', 'inference_results_batch', 'run_batch_inference.py', 'Train_BraTS_EDL.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "!pwd\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC: GI·∫¢I N√âN D·ªÆ LI·ªÜU RAW (QUAN TR·ªåNG CHO INFERENCE)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# 1. Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n (ƒê·∫£m b·∫£o bi·∫øn m√¥i tr∆∞·ªùng v·∫´n ƒë√∫ng)\n",
        "os.environ['nnUNet_raw'] = \"/content/nnUNet_raw\"\n",
        "!mkdir -p $nnUNet_raw\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n file zip tr√™n Drive (B·∫°n ki·ªÉm tra l·∫°i t√™n file cho ƒë√∫ng nh√©)\n",
        "# Gi·∫£ s·ª≠ b·∫°n l∆∞u l√† nnUNet_raw.zip trong th∆∞ m·ª•c XUM_project\n",
        "zip_path_drive = \"/content/drive/MyDrive/XUM_project/nnUNET_raw.zip\"\n",
        "zip_local = \"/content/nnUNet_raw.zip\"\n",
        "extract_root = \"/content/nnUNet_raw\"\n",
        "\n",
        "if not os.path.exists(zip_path_drive):\n",
        "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {zip_path_drive}\")\n",
        "    print(\"üëâ H√£y ki·ªÉm tra l·∫°i t√™n file zip raw tr√™n Google Drive c·ªßa b·∫°n.\")\n",
        "else:\n",
        "    print(\"üì¶ Copy file Raw Data t·ª´ Drive...\")\n",
        "    !cp \"$zip_path_drive\" \"$zip_local\"\n",
        "\n",
        "    print(\"üí® Copy xong. B·∫Øt ƒë·∫ßu gi·∫£i n√©n Raw Data...\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_local, 'r') as zip_ref:\n",
        "        # Detect folder root name inside zip (v√≠ d·ª•: nnUNet_raw/)\n",
        "        # ƒê·ªÉ tr√°nh b·ªã l·ªìng th∆∞ m·ª•c ki·ªÉu /content/nnUNet_raw/nnUNet_raw/Dataset...\n",
        "        root_name = zip_ref.namelist()[0].split('/')[0]\n",
        "\n",
        "        file_list = zip_ref.infolist()\n",
        "        total_files = len(file_list)\n",
        "\n",
        "        print(f\"üîß Ph√°t hi·ªán folder root trong ZIP: {root_name}\")\n",
        "        print(\"üîß ƒêang gi·∫£i n√©n v√† lo·∫°i b·ªè l·ªõp folder th·ª´a...\")\n",
        "\n",
        "        for file in tqdm(file_list, total=total_files, desc=\"üîÑ Unzipping Raw\"):\n",
        "            file_path = file.filename\n",
        "\n",
        "            # Logic: N·∫øu file n·∫±m trong folder root, ta b·ªè t√™n root ƒëi ƒë·ªÉ file nh·∫£y ra ngo√†i\n",
        "            # V√≠ d·ª•: nnUNet_raw/Dataset101/abc.nii -> Dataset101/abc.nii\n",
        "            if file_path.startswith(root_name + \"/\"):\n",
        "                fixed_path = file_path.replace(root_name + \"/\", \"\", 1)\n",
        "            else:\n",
        "                fixed_path = file_path # N·∫øu n√©n kh√¥ng c√≥ folder root th√¨ gi·ªØ nguy√™n\n",
        "\n",
        "            if fixed_path.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            # Ch·ªâ ƒë·ªãnh n∆°i l∆∞u ƒë√∫ng\n",
        "            target_path = os.path.join(extract_root, fixed_path)\n",
        "\n",
        "            # T·∫°o th∆∞ m·ª•c n·∫øu c·∫ßn\n",
        "            if file.is_dir():\n",
        "                os.makedirs(target_path, exist_ok=True)\n",
        "            else:\n",
        "                os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "                with zip_ref.open(file) as source, open(target_path, \"wb\") as target:\n",
        "                    target.write(source.read())\n",
        "\n",
        "    os.remove(zip_local)\n",
        "    print(\"üéâ ƒê√É GI·∫¢I N√âN RAW DATA TH√ÄNH C√îNG! S·∫¥N S√ÄNG CH·∫†Y INFERENCE! üöÄ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZP3YP9xcmf9",
        "outputId": "7781e47b-025a-45c8-929a-53151123a01b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "üì¶ Copy file Raw Data t·ª´ Drive...\n",
            "üí® Copy xong. B·∫Øt ƒë·∫ßu gi·∫£i n√©n Raw Data...\n",
            "üîß Ph√°t hi·ªán folder root trong ZIP: nnUNET_raw\n",
            "üîß ƒêang gi·∫£i n√©n v√† lo·∫°i b·ªè l·ªõp folder th·ª´a...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üîÑ Unzipping Raw: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1845/1845 [04:43<00:00,  6.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ ƒê√É GI·∫¢I N√âN RAW DATA TH√ÄNH C√îNG! S·∫¥N S√ÄNG CH·∫†Y INFERENCE! üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K7URZ28gwer_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce13b2de-48cf-4e43-da94-3b32a680f4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw: /content/nnUNet_raw\n",
            "Preprocessed: /content/nnUNet_preprocessed\n",
            "Results: /content/drive/MyDrive/XUM_project/nnUNet_results\n",
            "‚úÖ Tim thay file EDLTrainer.py!\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC 4: KI·ªÇM TRA BI·∫æN M√îI TR∆Ø·ªúNG & DATA\n",
        "# ----------------------------------------------------------------------\n",
        "print(f\"Raw: {os.environ['nnUNet_raw']}\")\n",
        "print(f\"Preprocessed: {os.environ['nnUNet_preprocessed']}\")\n",
        "print(f\"Results: {os.environ['nnUNet_results']}\")\n",
        "\n",
        "# Ki·ªÉm tra file EDLTrainer c√≥ t·ªìn t·∫°i kh√¥ng\n",
        "if os.path.exists(\"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/EDLTrainer.py\"):\n",
        "    print(\"‚úÖ Tim thay file EDLTrainer.py!\")\n",
        "else:\n",
        "    print(\"‚ùå KHONG TIM THAY EDLTrainer.py! Kiem tra lai file zip code.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat /content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/EDLTrainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NRxUfRHnPMw",
        "outputId": "648a32d9-0772-4469-a289-19e49fb12f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\r\n",
            "import torch.nn as nn\r\n",
            "import torch.nn.functional as F\r\n",
            "import numpy as np\r\n",
            "from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer\r\n",
            "from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss\r\n",
            "from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper\r\n",
            "\r\n",
            "# --- PH·∫¶N 1: H√ÄM LOSS EDL CHU·∫®N HO√Å ---\r\n",
            "class EDLLoss(nn.Module):\r\n",
            "    def __init__(self, num_classes, annealing_step=10, lamb=1.0):\r\n",
            "        super(EDLLoss, self).__init__()\r\n",
            "        self.num_classes = num_classes\r\n",
            "        self.annealing_step = annealing_step\r\n",
            "        self.lamb = lamb\r\n",
            "        \r\n",
            "        # Bi·∫øn n√†y s·∫Ω ƒë∆∞·ª£c update t·ª´ Trainer m·ªói epoch\r\n",
            "        self.current_epoch = 0 \r\n",
            "        \r\n",
            "        # Dice Loss: t·∫Øt do_bg (n·∫øu c·∫ßn), batch_dice=True ƒë·ªÉ ·ªïn ƒë·ªãnh\r\n",
            "        self.dice_loss = MemoryEfficientSoftDiceLoss(batch_dice=True, do_bg=False, smooth=1e-5, ddp=False)\r\n",
            "\r\n",
            "    def KL(self, alpha):\r\n",
            "        # KL Divergence: Dirichlet(alpha) || Dirichlet(1)\r\n",
            "        beta = torch.ones((1, self.num_classes) + alpha.shape[2:]).to(alpha.device)\r\n",
            "        S_alpha = torch.sum(alpha, dim=1, keepdim=True)\r\n",
            "        S_beta = torch.sum(beta, dim=1, keepdim=True)\r\n",
            "        \r\n",
            "        lnB = torch.lgamma(S_alpha) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True)\r\n",
            "        lnB_uni = torch.sum(torch.lgamma(beta), dim=1, keepdim=True) - torch.lgamma(S_beta)\r\n",
            "        \r\n",
            "        dg0 = torch.digamma(S_alpha)\r\n",
            "        dg1 = torch.digamma(alpha)\r\n",
            "        \r\n",
            "        kl = torch.sum((alpha - beta) * (dg1 - dg0), dim=1, keepdim=True) + lnB + lnB_uni\r\n",
            "        return kl\r\n",
            "\r\n",
            "    def forward(self, outputs, target):\r\n",
            "        \"\"\"\r\n",
            "        L∆ØU √ù: Kh√¥ng truy·ªÅn current_epoch v√†o argument c·ªßa forward \r\n",
            "        ƒë·ªÉ tr√°nh xung ƒë·ªôt v·ªõi DeepSupervisionWrapper.\r\n",
            "        Ta d√πng self.current_epoch ƒë√£ ƒë∆∞·ª£c g√°n t·ª´ b√™n ngo√†i.\r\n",
            "        \"\"\"\r\n",
            "        \r\n",
            "        # 1. One-hot encoding an to√†n cho nnU-Net (Target: [B, 1, X, Y, Z])\r\n",
            "        # Chuy·ªÉn target sang one-hot: [B, C, X, Y, Z]\r\n",
            "        if target.dim() == outputs.dim(): # N·∫øu target ƒë√£ c√≥ dim channel (th∆∞·ªùng l√† 1)\r\n",
            "            target = target.squeeze(1) # B·ªè dim channel t·∫°m th·ªùi ƒë·ªÉ d√πng one_hot\r\n",
            "            \r\n",
            "        # T·∫°o one-hot\r\n",
            "        target_one_hot = F.one_hot(target.long(), num_classes=self.num_classes)\r\n",
            "        # Permute ƒë·ªÉ ƒë∆∞a channel v·ªÅ ƒë√∫ng v·ªã tr√≠ s·ªë 2 (index 1): [B, X, Y, Z, C] -> [B, C, X, Y, Z]\r\n",
            "        target_one_hot = target_one_hot.permute(0, 4, 1, 2, 3).contiguous().type_as(outputs)\r\n",
            "\r\n",
            "        # 2. T√≠nh Evidence v√† Alpha\r\n",
            "        evidence = F.softplus(outputs)\r\n",
            "        alpha = evidence + 1\r\n",
            "        S = torch.sum(alpha, dim=1, keepdim=True)\r\n",
            "        \r\n",
            "        # 3. T√≠nh Bayes Risk (Cross Entropy c·∫£i bi√™n cho Dirichlet)\r\n",
            "        # D√πng Digamma thay v√¨ Log ƒë·ªÉ ƒë√∫ng to√°n h·ªçc h∆°n\r\n",
            "        # Loss = Sum( y * (digamma(S) - digamma(alpha)) )\r\n",
            "        edl_loss = torch.sum(target_one_hot * (torch.digamma(S) - torch.digamma(alpha)), dim=1, keepdim=True)\r\n",
            "        edl_loss = torch.mean(edl_loss)\r\n",
            "        \r\n",
            "        # 4. KL Divergence (Regularization)\r\n",
            "        # Annealing: TƒÉng d·∫ßn tr·ªçng s·ªë KL\r\n",
            "        annealing_coef = min(1.0, self.current_epoch / self.annealing_step)\r\n",
            "        \r\n",
            "        # Ch·ªâ t√≠nh KL cho c√°c pixel kh√¥ng ph·∫£i l√† Ground Truth (ƒë·ªÉ √©p evidence -> 0 ·ªü ch·ªó sai)\r\n",
            "        # Theo paper g·ªëc: KL_loss = KL(alpha || 1)\r\n",
            "        kl_alpha = (alpha - 1) * (1 - target_one_hot) + 1\r\n",
            "        kl_div = self.KL(kl_alpha)\r\n",
            "        kl_loss = annealing_coef * torch.mean(kl_div)\r\n",
            "        \r\n",
            "        # 5. Dice Loss\r\n",
            "        # Expected probability: p = alpha / S\r\n",
            "        p = alpha / S\r\n",
            "        # Dice loss c·ªßa nnU-Net c·∫ßn shape target g·ªëc [B, 1, X, Y, Z]\r\n",
            "        loss_dice = self.dice_loss(p, target.unsqueeze(1))\r\n",
            "        \r\n",
            "        # 6. T·ªïng h·ª£p\r\n",
            "        final_loss = edl_loss + kl_loss + self.lamb * loss_dice\r\n",
            "        return final_loss\r\n",
            "\r\n",
            "# --- PH·∫¶N 2: TRAINER ---\r\n",
            "class EDLTrainer(nnUNetTrainer):\r\n",
            "    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, unpack_dataset: bool = True,\r\n",
            "                 device: torch.device = torch.device('cuda')):\r\n",
            "        # FIX: B·ªè unpack_dataset khi g·ªçi super().__init__ v√¨ nnU-Net v2 m·ªõi ƒë√£ b·ªè tham s·ªë n√†y\r\n",
            "        super().__init__(plans, configuration, fold, dataset_json, device=device)\r\n",
            "        \r\n",
            "        # C√≥ th·ªÉ tƒÉng epoch n·∫øu c·∫ßn thi·∫øt\r\n",
            "        # self.num_epochs = 1000\r\n",
            "\r\n",
            "    def _build_loss(self):\r\n",
            "        num_classes = self.label_manager.num_segmentation_heads\r\n",
            "        \r\n",
            "        # Kh·ªüi t·∫°o EDLLoss\r\n",
            "        loss = EDLLoss(num_classes=num_classes, annealing_step=50, lamb=1.0)\r\n",
            "        \r\n",
            "        if self.enable_deep_supervision:\r\n",
            "            deep_supervision_scales = self._get_deep_supervision_scales()\r\n",
            "            weights = np.array([1 / (2 ** i) for i in range(len(deep_supervision_scales))])\r\n",
            "            weights[-1] = 0\r\n",
            "            weights = weights / weights.sum()\r\n",
            "            # Wrap loss\r\n",
            "            return DeepSupervisionWrapper(loss, weights)\r\n",
            "        \r\n",
            "        return loss\r\n",
            "\r\n",
            "    def train_step(self, batch: dict):\r\n",
            "        data = batch['data']\r\n",
            "        target = batch['target']\r\n",
            "        \r\n",
            "        data = data.to(self.device, non_blocking=True)\r\n",
            "        if isinstance(target, list):\r\n",
            "            target = [i.to(self.device, non_blocking=True) for i in target]\r\n",
            "        else:\r\n",
            "            target = target.to(self.device, non_blocking=True)\r\n",
            "            \r\n",
            "        self.optimizer.zero_grad()\r\n",
            "        \r\n",
            "        # --- UPDATE EPOCH CHO LOSS ---\r\n",
            "        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ x·ª≠ l√Ω Wrapper\r\n",
            "        if isinstance(self.loss, DeepSupervisionWrapper):\r\n",
            "            # N·∫øu b·ªã wrap, bi·∫øn loss th·ª±c s·ª± n·∫±m ·ªü self.loss.loss\r\n",
            "            self.loss.loss.current_epoch = self.current_epoch\r\n",
            "        else:\r\n",
            "            self.loss.current_epoch = self.current_epoch\r\n",
            "            \r\n",
            "        with torch.autocast(device_type=self.device.type, enabled=True):\r\n",
            "            output = self.network(data)\r\n",
            "            \r\n",
            "            # G·ªçi h√†m loss b√¨nh th∆∞·ªùng (kh√¥ng truy·ªÅn current_epoch v√†o ƒë√¢y n·ªØa)\r\n",
            "            l = self.loss(output, target)\r\n",
            "            \r\n",
            "        self.grad_scaler.scale(l).backward()\r\n",
            "        self.grad_scaler.unscale_(self.optimizer)\r\n",
            "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)\r\n",
            "        self.grad_scaler.step(self.optimizer)\r\n",
            "        self.grad_scaler.update()\r\n",
            "        \r\n",
            "        return {'loss': l.detach().cpu().numpy()}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MBcpWOcO15UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7738689d-49ac-48b5-cdea-ee283c3a256e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  Dataset101_BraTS2020\n"
          ]
        }
      ],
      "source": [
        "!ls nnUNet_preprocessed -a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training b·∫±ng `EDLTrainer`"
      ],
      "metadata": {
        "id": "fnr2_UYmdRmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDYz2Jqx0I5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e572a5-cd68-4054-f78d-a29728a59cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################\n",
            "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
            "############################\n",
            "\n",
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-11-22 13:01:22.781290: Using torch.compile...\n",
            "2025-11-22 13:01:27.235220: do_dummy_2d_data_aug: False\n",
            "2025-11-22 13:01:27.240783: Creating new 5-fold cross-validation split...\n",
            "2025-11-22 13:01:27.247902: Desired fold for training: 0\n",
            "2025-11-22 13:01:27.252467: This split has 294 training and 74 validation cases.\n",
            "using pin_memory on device 0\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [139.0, 170.0, 138.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [True, True, True, True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset101_BraTS2020', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [139, 170, 138], 'image_reader_writer': 'NibabelIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 29422.0, 'mean': 724.156982421875, 'median': 428.0, 'min': 0.0, 'percentile_00_5': 73.0, 'percentile_99_5': 16634.0, 'std': 1905.18603515625}, '1': {'max': 20094.0, 'mean': 644.5623779296875, 'median': 356.0, 'min': 0.0, 'percentile_00_5': 37.0, 'percentile_99_5': 8509.0, 'std': 1026.4256591796875}, '2': {'max': 18011.0, 'mean': 793.357421875, 'median': 415.0, 'min': 0.0, 'percentile_00_5': 36.0, 'percentile_99_5': 8117.0, 'std': 1158.4361572265625}, '3': {'max': 31404.0, 'mean': 1070.999267578125, 'median': 665.0, 'min': 0.0, 'percentile_00_5': 96.0, 'percentile_99_5': 16393.0, 'std': 1994.0704345703125}}} \n",
            "\n",
            "2025-11-22 13:01:36.137444: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-11-22 13:01:36.229325: \n",
            "2025-11-22 13:01:36.236921: Epoch 0\n",
            "2025-11-22 13:01:36.241736: Current learning rate: 0.01\n",
            "W1122 13:02:15.240000 9606 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "2025-11-22 13:14:34.486940: train_loss 0.2824\n",
            "2025-11-22 13:14:34.497862: val_loss -0.0621\n",
            "2025-11-22 13:14:34.501542: Pseudo dice [np.float32(0.1246), np.float32(0.6987), np.float32(0.7287)]\n",
            "2025-11-22 13:14:34.505131: Epoch time: 778.26 s\n",
            "2025-11-22 13:14:34.508479: Yayy! New best EMA pseudo Dice: 0.517300009727478\n",
            "2025-11-22 13:14:36.735356: \n",
            "2025-11-22 13:14:36.739322: Epoch 1\n",
            "2025-11-22 13:14:36.743268: Current learning rate: 0.00999\n",
            "2025-11-22 13:25:20.046252: train_loss -0.1227\n",
            "2025-11-22 13:25:20.071239: val_loss -0.2075\n",
            "2025-11-22 13:25:20.079488: Pseudo dice [np.float32(0.4538), np.float32(0.7395), np.float32(0.778)]\n",
            "2025-11-22 13:25:20.089611: Epoch time: 643.31 s\n",
            "2025-11-22 13:25:20.098206: Yayy! New best EMA pseudo Dice: 0.5313000082969666\n",
            "2025-11-22 13:25:24.945412: \n",
            "2025-11-22 13:25:24.972489: Epoch 2\n",
            "2025-11-22 13:25:24.977571: Current learning rate: 0.00998\n",
            "2025-11-22 13:37:37.179947: train_loss -0.217\n",
            "2025-11-22 13:37:37.211301: val_loss -0.2835\n",
            "2025-11-22 13:37:37.221139: Pseudo dice [np.float32(0.4965), np.float32(0.7881), np.float32(0.7893)]\n",
            "2025-11-22 13:37:37.234169: Epoch time: 732.24 s\n",
            "2025-11-22 13:37:37.248218: Yayy! New best EMA pseudo Dice: 0.5472999811172485\n",
            "2025-11-22 13:37:47.866905: \n",
            "2025-11-22 13:37:47.870102: Epoch 3\n",
            "2025-11-22 13:37:47.876777: Current learning rate: 0.00997\n",
            "2025-11-22 13:49:53.792387: train_loss -0.275\n",
            "2025-11-22 13:49:53.816850: val_loss -0.331\n",
            "2025-11-22 13:49:53.827195: Pseudo dice [np.float32(0.5731), np.float32(0.8112), np.float32(0.832)]\n",
            "2025-11-22 13:49:53.841017: Epoch time: 725.93 s\n",
            "2025-11-22 13:49:53.854016: Yayy! New best EMA pseudo Dice: 0.5663999915122986\n",
            "2025-11-22 13:50:01.006842: \n",
            "2025-11-22 13:50:01.016865: Epoch 4\n",
            "2025-11-22 13:50:01.039614: Current learning rate: 0.00996\n",
            "2025-11-22 14:02:06.089681: train_loss -0.3214\n",
            "2025-11-22 14:02:06.118772: val_loss -0.3437\n",
            "2025-11-22 14:02:06.138200: Pseudo dice [np.float32(0.5945), np.float32(0.7956), np.float32(0.8254)]\n",
            "2025-11-22 14:02:06.171666: Epoch time: 725.09 s\n",
            "2025-11-22 14:02:06.217067: Yayy! New best EMA pseudo Dice: 0.5837000012397766\n",
            "2025-11-22 14:02:13.429549: \n",
            "2025-11-22 14:02:13.432901: Epoch 5\n",
            "2025-11-22 14:02:13.436677: Current learning rate: 0.00995\n",
            "2025-11-22 14:14:30.455962: train_loss -0.3439\n",
            "2025-11-22 14:14:30.489996: val_loss -0.3737\n",
            "2025-11-22 14:14:30.510478: Pseudo dice [np.float32(0.6448), np.float32(0.7807), np.float32(0.8353)]\n",
            "2025-11-22 14:14:30.521201: Epoch time: 737.03 s\n",
            "2025-11-22 14:14:30.529475: Yayy! New best EMA pseudo Dice: 0.600600004196167\n",
            "2025-11-22 14:14:39.530239: \n",
            "2025-11-22 14:14:39.548198: Epoch 6\n",
            "2025-11-22 14:14:39.552555: Current learning rate: 0.00995\n",
            "2025-11-22 14:27:05.068136: train_loss -0.3634\n",
            "2025-11-22 14:27:05.085896: val_loss -0.3348\n",
            "2025-11-22 14:27:05.094517: Pseudo dice [np.float32(0.6069), np.float32(0.7138), np.float32(0.831)]\n",
            "2025-11-22 14:27:05.100519: Epoch time: 745.54 s\n",
            "2025-11-22 14:27:05.107846: Yayy! New best EMA pseudo Dice: 0.6122999787330627\n",
            "2025-11-22 14:27:14.447962: \n",
            "2025-11-22 14:27:14.482608: Epoch 7\n",
            "2025-11-22 14:27:14.531252: Current learning rate: 0.00994\n",
            "2025-11-22 14:39:47.281090: train_loss -0.3705\n",
            "2025-11-22 14:39:47.312871: val_loss -0.4156\n",
            "2025-11-22 14:39:47.330897: Pseudo dice [np.float32(0.7035), np.float32(0.8213), np.float32(0.848)]\n",
            "2025-11-22 14:39:47.345678: Epoch time: 752.84 s\n",
            "2025-11-22 14:39:47.365174: Yayy! New best EMA pseudo Dice: 0.6302000284194946\n",
            "2025-11-22 14:39:54.675339: \n",
            "2025-11-22 14:39:54.695853: Epoch 8\n",
            "2025-11-22 14:39:54.715939: Current learning rate: 0.00993\n",
            "2025-11-22 14:52:33.905182: train_loss -0.408\n",
            "2025-11-22 14:52:33.916515: val_loss -0.4179\n",
            "2025-11-22 14:52:33.924113: Pseudo dice [np.float32(0.692), np.float32(0.8077), np.float32(0.8491)]\n",
            "2025-11-22 14:52:33.930309: Epoch time: 759.24 s\n",
            "2025-11-22 14:52:33.937795: Yayy! New best EMA pseudo Dice: 0.6453999876976013\n",
            "2025-11-22 14:52:45.092669: \n",
            "2025-11-22 14:52:45.098338: Epoch 9\n",
            "2025-11-22 14:52:45.105953: Current learning rate: 0.00992\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nnUNetv2_train\", line 8, in <module>\n",
            "    sys.exit(run_training_entry())\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/run/run_training.py\", line 266, in run_training_entry\n",
            "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/run/run_training.py\", line 207, in run_training\n",
            "    nnunet_trainer.run_training()\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1371, in run_training\n",
            "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/EDLTrainer.py\", line 91, in train_step\n",
            "    l = self.loss(output, target)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/loss/deep_supervision.py\", line 29, in forward\n",
            "    return sum([weights[i] * self.loss(*inputs) for i, inputs in enumerate(zip(*args)) if weights[i] != 0.0])\n",
            "                             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/EDLTrainer.py\", line 42, in forward\n",
            "    kl_div = self.KL(kl_alpha)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/training/nnUNetTrainer/EDLTrainer.py\", line 20, in KL\n",
            "    beta = torch.ones((1, self.num_classes) + alpha.shape[2:]).to(alpha.device)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# B∆Ø·ªöC 5: B·∫ÆT ƒê·∫¶U TRAINING (The Moment of Truth)\n",
        "# ----------------------------------------------------------------------\n",
        "# L·ªánh train:\n",
        "# Dataset101_BraTS2020: T√™n dataset\n",
        "# 3d_fullres: C·∫•u h√¨nh\n",
        "# 0: Fold (th∆∞·ªùng ch·∫°y fold 0 tr∆∞·ªõc)\n",
        "# -tr EDLTrainer: D√πng Trainer Custom c·ªßa b·∫°n\n",
        "# --npz: L∆∞u x√°c su·∫•t softmax (ƒë·ªÉ d√πng cho validation sau n√†y)\n",
        "\n",
        "# Khai b√°o bi·∫øn m√¥i tr∆∞·ªùng tr·ª±c ti·∫øp tr√™n c√πng 1 d√≤ng l·ªánh ƒë·ªÉ ƒë·∫£m b·∫£o nnU-Net nh·∫≠n di·ªán ƒë∆∞·ª£c\n",
        "!nnUNet_raw=\"/content/nnUNet_raw\" \\\n",
        "nnUNet_preprocessed=\"/content/nnUNet_preprocessed\" \\\n",
        "nnUNet_results=\"/content/drive/MyDrive/XUM_project/nnUNet_results\" \\\n",
        "nnUNetv2_train Dataset101_BraTS2020 3d_fullres 0 -tr EDLTrainer --npz\n",
        "\n",
        "# Sao l∆∞u file split quan tr·ªçng v·ªÅ Drive ƒë·ªÉ d√πng cho Inference sau n√†y\n",
        "!cp \"/content/nnUNet_preprocessed/Dataset101_BraTS2020/splits_final.json\" \"/content/drive/MyDrive/BraTS2020_EDL_Project/\"\n",
        "\n",
        "print(\"‚úÖ ƒê√£ backup splits_final.json v·ªÅ Drive an to√†n!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy `dataset.json` t·ª´ `nnUNet_results/Dataset101_BraTS2020/EDLTrainer__nnUNetPlans__3d_fullres` v√¥ `nnUNet_results/Dataset101_BraTS2020/EDLTrainer__nnUNetPlans__3d_fullres/fold_0`"
      ],
      "metadata": {
        "id": "sT-ehISDjO4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q7htJFm71d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f641a3-5edc-434f-bfb5-be7464987f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/inference_edl.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# !python inference_edl.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo testing"
      ],
      "metadata": {
        "id": "PkqgKzlt2ki8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from batchgenerators.utilities.file_and_folder_operations import save_json\n",
        "\n",
        "def regenerate_splits():\n",
        "    # 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
        "    preprocessed_dir = \"/content/nnUNet_preprocessed/Dataset101_BraTS2020\"\n",
        "    splits_file = os.path.join(preprocessed_dir, \"splits_final.json\")\n",
        "    gt_folder = os.path.join(preprocessed_dir, \"gt_segmentations\")\n",
        "\n",
        "    if os.path.exists(splits_file):\n",
        "        print(\"‚úÖ File splits_final.json ƒë√£ t·ªìn t·∫°i. Kh√¥ng c·∫ßn t·∫°o l·∫°i.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y splits_final.json. ƒêang t√°i t·∫°o l·∫°i theo chu·∫©n nnU-Net...\")\n",
        "\n",
        "    # 2. L·∫•y danh s√°ch t·∫•t c·∫£ Case ID t·ª´ th∆∞ m·ª•c Ground Truth ƒë√£ preprocess\n",
        "    if not os.path.exists(gt_folder):\n",
        "        raise FileNotFoundError(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y folder {gt_folder}. B·∫°n ƒë√£ gi·∫£i n√©n data ch∆∞a?\")\n",
        "\n",
        "    files = sorted([f for f in os.listdir(gt_folder) if f.endswith(\".nii.gz\") or f.endswith(\".nii\")])\n",
        "    identifiers = [f.replace(\".nii.gz\", \"\").replace(\".nii\", \"\") for f in files]\n",
        "\n",
        "    print(f\"üìÇ T√¨m th·∫•y {len(identifiers)} ca b·ªánh trong t·∫≠p d·ªØ li·ªáu.\")\n",
        "\n",
        "    # 3. T√°i t·∫°o Split (S·ª≠ d·ª•ng thu·∫≠t to√°n K-Fold c·ªßa nnU-Net)\n",
        "    # Code n√†y m√¥ ph·ªèng l·∫°i h√†m generate_crossval_split c·ªßa nnU-Net\n",
        "    seed = 12345\n",
        "    n_splits = 5\n",
        "\n",
        "    kfold = []\n",
        "\n",
        "    # X√°o tr·ªôn danh s√°ch v·ªõi seed c·ªë ƒë·ªãnh ƒë·ªÉ ƒë·∫£m b·∫£o gi·ªëng h·ªát l·∫ßn train tr∆∞·ªõc\n",
        "    rs = np.random.RandomState(seed)\n",
        "    keys = np.unique(identifiers)\n",
        "    rs.shuffle(keys)\n",
        "\n",
        "    # Chia k-fold\n",
        "    k = n_splits\n",
        "    for i in range(k):\n",
        "        # Logic chia: fold i s·∫Ω l√† t·∫≠p val, c√°c fold c√≤n l·∫°i l√† train\n",
        "        val_keys = keys[i::k]\n",
        "        train_keys = [x for x in keys if x not in val_keys]\n",
        "\n",
        "        kfold.append({\n",
        "            'train': list(train_keys),\n",
        "            'val': list(val_keys)\n",
        "        })\n",
        "\n",
        "    # 4. L∆∞u file\n",
        "    save_json(kfold, splits_file)\n",
        "    print(f\"üéâ ƒê√£ t·∫°o xong file: {splits_file}\")\n",
        "    print(\"üëâ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i file run_batch_inference.py!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    regenerate_splits()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpr96yGESqM1",
        "outputId": "f90ab8e7-ab4f-403d-9b1f-29bdb85f0e44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y splits_final.json. ƒêang t√°i t·∫°o l·∫°i theo chu·∫©n nnU-Net...\n",
            "üìÇ T√¨m th·∫•y 368 ca b·ªánh trong t·∫≠p d·ªØ li·ªáu.\n",
            "üéâ ƒê√£ t·∫°o xong file: /content/nnUNet_preprocessed/Dataset101_BraTS2020/splits_final.json\n",
            "üëâ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i file run_batch_inference.py!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/XUM_project/run_batch_inference.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE6orrX1h-l5",
        "outputId": "d0c625a8-1050-4cdc-ff24-939fe7abdb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ --- BATCH INFERENCE STARTED ---\n",
            "üìÇ Model Loaded from: /content/drive/MyDrive/XUM_project/nnUNet_results/Dataset101_BraTS2020/EDLTrainer__nnUNetPlans__3d_fullres\n",
            "üìÇ Found total 368 cases in raw folder.\n",
            "üìÇ ƒê√£ load danh s√°ch Validation Fold 0: 74 ca.\n",
            "‚öôÔ∏è Mode: VALIDATION SPLIT (Fold 0) -> Running 74 cases.\n",
            "\n",
            "üîç Processing: BRATS_039...\n",
            "100% 4/4 [00:08<00:00,  2.12s/it]\n",
            "    üì∏ Drawing Slice: 96\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_039_slice96.png\n",
            "\n",
            "üîç Processing: BRATS_153...\n",
            "100% 8/8 [00:16<00:00,  2.07s/it]\n",
            "    üì∏ Drawing Slice: 109\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_153_slice109.png\n",
            "\n",
            "üîç Processing: BRATS_299...\n",
            "100% 8/8 [00:16<00:00,  2.10s/it]\n",
            "    üì∏ Drawing Slice: 103\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_299_slice103.png\n",
            "\n",
            "üîç Processing: BRATS_155...\n",
            "100% 8/8 [00:17<00:00,  2.16s/it]\n",
            "    üì∏ Drawing Slice: 87\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_155_slice87.png\n",
            "\n",
            "üîç Processing: BRATS_074...\n",
            "100% 8/8 [00:17<00:00,  2.17s/it]\n",
            "    üì∏ Drawing Slice: 94\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_074_slice94.png\n",
            "\n",
            "üîç Processing: BRATS_111...\n",
            "100% 8/8 [00:17<00:00,  2.23s/it]\n",
            "    üì∏ Drawing Slice: 31\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_111_slice31.png\n",
            "\n",
            "üîç Processing: BRATS_181...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 86\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_181_slice86.png\n",
            "\n",
            "üîç Processing: BRATS_226...\n",
            "100% 8/8 [00:17<00:00,  2.18s/it]\n",
            "    üì∏ Drawing Slice: 97\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_226_slice97.png\n",
            "\n",
            "üîç Processing: BRATS_301...\n",
            "100% 8/8 [00:17<00:00,  2.19s/it]\n",
            "    üì∏ Drawing Slice: 82\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_301_slice82.png\n",
            "\n",
            "üîç Processing: BRATS_352...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 30\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_352_slice30.png\n",
            "\n",
            "üîç Processing: BRATS_056...\n",
            "100% 4/4 [00:08<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 50\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_056_slice50.png\n",
            "\n",
            "üîç Processing: BRATS_137...\n",
            "100% 8/8 [00:17<00:00,  2.22s/it]\n",
            "    üì∏ Drawing Slice: 40\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_137_slice40.png\n",
            "\n",
            "üîç Processing: BRATS_042...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 94\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_042_slice94.png\n",
            "\n",
            "üîç Processing: BRATS_011...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 111\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_011_slice111.png\n",
            "\n",
            "üîç Processing: BRATS_354...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 46\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_354_slice46.png\n",
            "\n",
            "üîç Processing: BRATS_305...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 51\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_305_slice51.png\n",
            "\n",
            "üîç Processing: BRATS_136...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 55\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_136_slice55.png\n",
            "\n",
            "üîç Processing: BRATS_055...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 115\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_055_slice115.png\n",
            "\n",
            "üîç Processing: BRATS_017...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 115\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_017_slice115.png\n",
            "\n",
            "üîç Processing: BRATS_129...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 36\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_129_slice36.png\n",
            "\n",
            "üîç Processing: BRATS_198...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 33\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_198_slice33.png\n",
            "\n",
            "üîç Processing: BRATS_076...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 47\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_076_slice47.png\n",
            "\n",
            "üîç Processing: BRATS_339...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 48\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_339_slice48.png\n",
            "\n",
            "üîç Processing: BRATS_293...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 47\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_293_slice47.png\n",
            "\n",
            "üîç Processing: BRATS_159...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 36\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_159_slice36.png\n",
            "\n",
            "üîç Processing: BRATS_009...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 41\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_009_slice41.png\n",
            "\n",
            "üîç Processing: BRATS_135...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 85\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_135_slice85.png\n",
            "\n",
            "üîç Processing: BRATS_121...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 103\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_121_slice103.png\n",
            "\n",
            "üîç Processing: BRATS_178...\n",
            "100% 8/8 [00:17<00:00,  2.21s/it]\n",
            "    üì∏ Drawing Slice: 34\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_178_slice34.png\n",
            "\n",
            "üîç Processing: BRATS_142...\n",
            "100% 4/4 [00:08<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 42\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_142_slice42.png\n",
            "\n",
            "üîç Processing: BRATS_238...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 99\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_238_slice99.png\n",
            "\n",
            "üîç Processing: BRATS_008...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 34\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_008_slice34.png\n",
            "\n",
            "üîç Processing: BRATS_010...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 25\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_010_slice25.png\n",
            "\n",
            "üîç Processing: BRATS_350...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 50\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_350_slice50.png\n",
            "\n",
            "üîç Processing: BRATS_256...\n",
            "100% 8/8 [00:17<00:00,  2.20s/it]\n",
            "    üì∏ Drawing Slice: 38\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_256_slice38.png\n",
            "\n",
            "üîç Processing: BRATS_314...\n",
            " 25% 2/8 [00:04<00:13,  2.25s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/XUM_project/run_batch_inference.py\", line 221, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/XUM_project/run_batch_inference.py\", line 212, in main\n",
            "    process_case(predictor, case_id)\n",
            "  File \"/content/drive/MyDrive/XUM_project/run_batch_inference.py\", line 139, in process_case\n",
            "    pred_logits = predictor.predict_logits_from_preprocessed_data(data_tensor)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/inference/predict_from_raw_data.py\", line 495, in predict_logits_from_preprocessed_data\n",
            "    prediction = self.predict_sliding_window_return_logits(data).to('cpu')\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/inference/predict_from_raw_data.py\", line 666, in predict_sliding_window_return_logits\n",
            "    predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/inference/predict_from_raw_data.py\", line 609, in _internal_predict_sliding_window_return_logits\n",
            "    prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nnUNet_Workspace/nnUNet/nnunetv2/inference/predict_from_raw_data.py\", line 543, in _internal_maybe_mirror_and_predict\n",
            "    prediction = self.network(x)\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/dynamic_network_architectures/architectures/unet.py\", line 93, in forward\n",
            "    skips = self.encoder(x)\n",
            "            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/dynamic_network_architectures/building_blocks/plain_conv_encoder.py\", line 86, in forward\n",
            "    x = s(x)\n",
            "        ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/dynamic_network_architectures/building_blocks/simple_conv_blocks.py\", line 137, in forward\n",
            "    return self.convs(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/dynamic_network_architectures/building_blocks/simple_conv_blocks.py\", line 71, in forward\n",
            "    return self.all_modules(x)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 717, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 712, in _conv_forward\n",
            "    return F.conv3d(\n",
            "           ^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVqLPZLQSwGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4a7AvaLMSwNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python \"/content/drive/MyDrive/XUM_project/run_batch_inference.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfIPZ3_b2n3F",
        "outputId": "a6f43304-9e4f-4067-e7f1-2bb1a628e2eb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ --- BATCH INFERENCE STARTED ---\n",
            "üìÇ Model Loaded from: /content/drive/MyDrive/XUM_project/nnUNet_results/Dataset101_BraTS2020/EDLTrainer__nnUNetPlans__3d_fullres\n",
            "üìÇ Found total 368 cases in raw folder.\n",
            "‚öôÔ∏è Mode: RANGE [186:190] -> Running 4 cases.\n",
            "\n",
            "üîç Processing: BRATS_187...\n",
            "100% 4/4 [00:08<00:00,  2.06s/it]\n",
            "    üì∏ Drawing Slice: 38\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_187_slice38.png\n",
            "\n",
            "üîç Processing: BRATS_188...\n",
            "100% 8/8 [00:15<00:00,  2.00s/it]\n",
            "    üì∏ Drawing Slice: 46\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_188_slice46.png\n",
            "\n",
            "üîç Processing: BRATS_189...\n",
            "100% 8/8 [00:16<00:00,  2.05s/it]\n",
            "    üì∏ Drawing Slice: 54\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_189_slice54.png\n",
            "\n",
            "üîç Processing: BRATS_190...\n",
            "100% 8/8 [00:16<00:00,  2.05s/it]\n",
            "    üì∏ Drawing Slice: 117\n",
            "    ‚úÖ Saved: /content/drive/MyDrive/XUM_project/inference_results_batch/BRATS_190_slice117.png\n",
            "\n",
            "‚úÖ --- BATCH INFERENCE FINISHED ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEZVqPDb7PUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}