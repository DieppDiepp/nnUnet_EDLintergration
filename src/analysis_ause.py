"""
üöÄ MAIN ANALYSIS SCRIPT (REFACTORED V13 - FINAL QU-BRATS STANDARD)
Ch·∫°y ph√¢n t√≠ch QU-BraTS Score v√† AUSE v·ªõi chu·∫©n h√≥a 0-100.
T√≠ch h·ª£p logic t√≠nh to√°n ch√≠nh x√°c v√† v·∫Ω bi·ªÉu ƒë·ªì.
"""
import sys
import os
import argparse
import numpy as np
from tqdm import tqdm
import pandas as pd
import warnings

# --- SETUP PATH ---
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path: sys.path.append(current_dir)
    # Th√™m root project
    project_root = os.path.dirname(current_dir)
    if project_root not in sys.path: sys.path.append(project_root)
except NameError: pass

# --- IMPORTS ---
try:
    from src.config import BASE_CONFIG, MODEL_CONFIGS
    from src.analysis.utils import load_nifti_safe, get_binary_mask
    from src.analysis.metrics import compute_metrics_by_thresholds, calculate_auc_score
    # L∆∞u √Ω: C·∫ßn c·∫≠p nh·∫≠t plotting.py ƒë·ªÉ h·ªó tr·ª£ v·∫Ω theo threshold n·∫øu mu·ªën, 
    # nh∆∞ng ·ªü ƒë√¢y ta s·∫Ω d√πng matplotlib tr·ª±c ti·∫øp cho ƒë∆°n gi·∫£n ho·∫∑c c·∫≠p nh·∫≠t sau.
    # T·∫°m th·ªùi ta s·∫Ω v·∫Ω tr·ª±c ti·∫øp trong file n√†y ƒë·ªÉ ki·ªÉm so√°t t·ªët h∆°n.
except ImportError as e:
    print(f"‚ùå Import Error: {e}")
    print("üí° G·ª£i √Ω: ƒê·∫£m b·∫£o b·∫°n ƒë√£ t·∫°o ƒë√∫ng c·∫•u tr√∫c folder src/analysis/...")
    sys.exit(1)

import matplotlib.pyplot as plt # Import l·∫°i ƒë·ªÉ v·∫Ω

warnings.filterwarnings("ignore")

def normalize_uncertainty(unc_map):
    """Chu·∫©n h√≥a Uncertainty v·ªÅ kho·∫£ng [0, 100]"""
    u_min = unc_map.min()
    u_max = unc_map.max()
    if u_max == u_min:
        return np.zeros_like(unc_map)
    # C√¥ng th·ª©c Min-Max Scaling * 100
    return ((unc_map - u_min) / (u_max - u_min)) * 100.0

def run_analysis_pipeline(mode='edl', n_cases=None):
    print(f"üöÄ STARTING ANALYSIS (QU-BRATS STANDARD 0-100) | MODE: {mode.upper()}")
    
    # 1. Config & Paths
    try:
        model_cfg = MODEL_CONFIGS[mode]
        base_folder = model_cfg["output_folder"]
        nifti_dir = os.path.join(base_folder, BASE_CONFIG.get("dir_nifti", "3d_nifti"))
        output_dir = os.path.join(base_folder, "analysis_qu_brats_v13") # Version m·ªõi
        os.makedirs(output_dir, exist_ok=True)
        
        if not os.path.exists(nifti_dir): raise FileNotFoundError(f"Input missing: {nifti_dir}")
    except Exception as e:
        print(f"‚ùå Config Error: {e}"); return

    all_cases = sorted([d for d in os.listdir(nifti_dir) if os.path.isdir(os.path.join(nifti_dir, d))])
    if n_cases: all_cases = all_cases[:n_cases]
    print(f"üîç Found {len(all_cases)} cases.")

    final_report = []
    TARGET_CLASSES = ["WT", "TC", "ET"]
    
    # ƒê·ªãnh nghƒ©a c√°c ng∆∞·ª°ng Threshold (0-100)
    THRESHOLDS = np.arange(100, -1, -5) # 100, 95, ..., 0

    # 2. Main Loop
    for target in TARGET_CLASSES:
        print(f"\nüìä Class: {target}")
        
        agg_data = {
            "total_dice": [], "total_ftp": [], "total_ftn": [],
            "aleatoric_dice": [], "epistemic_dice": []
        }

        for case_id in tqdm(all_cases, desc=f"Processing"):
            try:
                case_path = os.path.join(nifti_dir, case_id)
                pred = load_nifti_safe(os.path.join(case_path, "prediction.nii.gz"))
                gt = load_nifti_safe(os.path.join(case_path, "ground_truth.nii.gz"))
                mri = load_nifti_safe(os.path.join(case_path, "mri_crop.nii.gz"))

                if pred is None or gt is None: continue

                # Brain Masking
                brain_mask = (mri > 0) if mri is not None else np.logical_or(pred > 0, gt > 0)
                if brain_mask.sum() == 0: continue

                pred_roi = pred[brain_mask]
                gt_roi = gt[brain_mask]

                # --- Total Uncertainty ---
                u_total = load_nifti_safe(os.path.join(case_path, "unc_total.nii.gz"))
                if u_total is not None:
                    # 1. Chu·∫©n h√≥a
                    u_norm = normalize_uncertainty(u_total[brain_mask])
                    
                    # 2. T√≠nh theo Thresholds (G·ªçi h√†m t·ª´ metrics.py)
                    res = compute_metrics_by_thresholds(
                        pred_roi, gt_roi, u_norm, target, THRESHOLDS
                    )
                    
                    if res is not None:
                        agg_data["total_dice"].append(res["dice"])
                        agg_data["total_ftp"].append(res["ftp"])
                        agg_data["total_ftn"].append(res["ftn"])

                # --- Components ---
                for u_name in ["aleatoric", "epistemic"]:
                    u_map = load_nifti_safe(os.path.join(case_path, f"unc_{u_name}.nii.gz"))
                    if u_map is not None:
                        u_norm = normalize_uncertainty(u_map[brain_mask])
                        res = compute_metrics_by_thresholds(
                            pred_roi, gt_roi, u_norm, target, THRESHOLDS
                        )
                        if res is not None: agg_data[f"{u_name}_dice"].append(res["dice"])

            except Exception: continue

        if not agg_data["total_dice"]: 
            print(f"‚ö†Ô∏è No data for {target}")
            continue

        # 3. Aggregation & Metrics
        # T√≠nh trung b√¨nh theo c·ªôt (axis=0)
        mean_dice = np.mean(agg_data["total_dice"], axis=0)
        mean_ftp = np.mean(agg_data["total_ftp"], axis=0)
        mean_ftn = np.mean(agg_data["total_ftn"], axis=0)

        # QU-BraTS Score Calculation
        # L∆∞u √Ω: x_axis ·ªü ƒë√¢y l√† THRESHOLDS / 100 ƒë·ªÉ v·ªÅ range [0, 1] cho AUC
        # Nh∆∞ng THRESHOLDS ƒëang gi·∫£m d·∫ßn (100 -> 0), AUC c·∫ßn x tƒÉng d·∫ßn -> ƒê·∫£o chi·ªÅu
        x_norm = THRESHOLDS[::-1] / 100.0 
        
        auc_dice = calculate_auc_score(x_norm, mean_dice[::-1])
        auc_ftp = calculate_auc_score(x_norm, mean_ftp[::-1])
        auc_ftn = calculate_auc_score(x_norm, mean_ftn[::-1])
        
        qu_score = (auc_dice + (1 - auc_ftp) + (1 - auc_ftn)) / 3.0
        
        # Component AUCs (Ch·ªâ t√≠nh Dice AUC ƒë·ªÉ so s√°nh)
        auc_dice_alea = 0
        if agg_data["aleatoric_dice"]:
            mean_alea = np.mean(agg_data["aleatoric_dice"], axis=0)
            auc_dice_alea = calculate_auc_score(x_norm, mean_alea[::-1])
            
        auc_dice_epis = 0
        if agg_data["epistemic_dice"]:
            mean_epis = np.mean(agg_data["epistemic_dice"], axis=0)
            auc_dice_epis = calculate_auc_score(x_norm, mean_epis[::-1])

        final_report.append({
            "Class": target, 
            "QU_Score": qu_score,
            "AUC_Dice_Total": auc_dice,
            "AUC_Dice_Aleatoric": auc_dice_alea,
            "AUC_Dice_Epistemic": auc_dice_epis,
            "AUC_FTP": auc_ftp,
            "AUC_FTN": auc_ftn
        })

        # 4. Plotting (V·∫Ω tr·ª±c ti·∫øp t·∫°i ƒë√¢y)
        plt.figure(figsize=(10, 6))
        plt.plot(THRESHOLDS, mean_dice, 'r-', label=f'Total (AUC={auc_dice:.3f})', linewidth=2)
        
        if agg_data["aleatoric_dice"]:
            mean_alea = np.mean(agg_data["aleatoric_dice"], axis=0)
            plt.plot(THRESHOLDS, mean_alea, 'g:', label=f'Aleatoric (AUC={auc_dice_alea:.3f})')
            
        if agg_data["epistemic_dice"]:
            mean_epis = np.mean(agg_data["epistemic_dice"], axis=0)
            plt.plot(THRESHOLDS, mean_epis, 'b-.', label=f'Epistemic (AUC={auc_dice_epis:.3f})')

        plt.xlabel("Uncertainty Threshold (œÑ)", fontsize=12)
        plt.ylabel(f"Dice Score ({target})", fontsize=12)
        plt.title(f"Performance vs. Uncertainty Threshold ({target})\nScore={qu_score:.3f}", fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.gca().invert_xaxis() # 100 -> 0
        
        plt.savefig(os.path.join(output_dir, f"qu_brats_curve_{target}.png"), dpi=300)
        plt.close()

    # 5. Final Report
    print("\n" + "="*80)
    print(f"{'üèÜ QU-BRATS METRICS REPORT (V13)':^80}")
    print("="*80)
    df = pd.DataFrame(final_report)
    if not df.empty:
        cols = ["Class", "QU_Score", "AUC_Dice_Total", "AUC_FTP", "AUC_FTN"]
        print(df[cols].to_string(index=False, float_format="%.4f"))
        df.to_csv(os.path.join(output_dir, "final_qu_metrics.csv"), index=False)
        print(f"\n‚úÖ Results saved to: {output_dir}")
    else:
        print("‚ùå No results computed.")
    print("-" * 80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, default='edl')
    parser.add_argument('--limit', type=int, default=0)
    if 'ipykernel' in sys.modules: args = parser.parse_args([])
    else: args = parser.parse_args()
    
    run_analysis_pipeline(mode=args.mode, n_cases=args.limit)