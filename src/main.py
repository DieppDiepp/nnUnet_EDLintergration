"""
üöÄ MAIN SCRIPT (UPDATED V6 - FINAL)
In ra log chi ti·∫øt 3 ch·ªâ s·ªë BraTS (WT, TC, ET) v·ªõi ƒë·ªãnh d·∫°ng b·∫£ng ƒë·∫πp m·∫Øt.
ƒê·∫£m b·∫£o an to√†n tuy·ªát ƒë·ªëi (Robust Error Handling).
"""
import sys
import os
import random
import pandas as pd
import numpy as np

sys.path.append("/content/drive/MyDrive/XUM_project")

from src.config import CONFIG
from src.utils import get_case_list, get_validation_cases, calculate_metric_per_class
from src.edl_engine import EDLInferenceEngine
from src.visualizer import visualize_comparison 

def main():
    print("üèÅ --- STARTING EDL PIPELINE (BRATS REGIONS) ---")
    engine = EDLInferenceEngine(CONFIG)
    
    # --- 1. L·∫¨P DANH S√ÅCH CASE ---
    run_mode = CONFIG["run_mode"]
    all_cases_on_disk = get_case_list(CONFIG["image_folder"])
    
    if run_mode == "validation_split":
        cases = get_validation_cases(CONFIG["split_file"], fold=CONFIG["fold"])
        available_set = set(all_cases_on_disk)
        cases = [c for c in cases if c in available_set]
        print(f"‚öôÔ∏è Mode: VALIDATION SPLIT -> Found {len(cases)} cases.")
    elif run_mode == "range":
        start, end = CONFIG["test_range"]
        cases = all_cases_on_disk[start:end]
        print(f"‚öôÔ∏è Mode: RANGE [{start}:{end}] -> {len(cases)} cases.")
    else:
        num_rnd = CONFIG.get("num_random", 5)
        cases = random.sample(all_cases_on_disk, min(len(all_cases_on_disk), num_rnd))
        print(f"‚öôÔ∏è Mode: RANDOM -> {len(cases)} cases.")

    # --- 2. V√íNG L·∫∂P X·ª¨ L√ù ---
    all_metrics = []
    
    # In Header b·∫£ng
    print("\n" + "="*85)
    print(f"{'Index':<8} | {'Case ID':<15} | {'Dice WT':<8} | {'Dice TC':<8} | {'Dice ET':<8} | {'Mean':<8}")
    print("-" * 85)

    for i, case_id in enumerate(cases):
        try:
            # Process (Tr·∫£ v·ªÅ unc_dict)
            mri, gt, pred, unc_dict, props = engine.process_case(case_id)
            
            # [QUAN TR·ªåNG] Ki·ªÉm tra an to√†n: N·∫øu l·ªói load file -> B·ªè qua
            if mri is None:
                print(f"{i+1:<8} | {case_id:<15} | {'SKIPPED (Error)':<40}")
                continue

            if CONFIG["calc_metrics"]:
                spacing = props.get('spacing', None)
                # gt[0] v√¨ gt shape l√† (1, X, Y, Z)
                metrics = calculate_metric_per_class(pred, gt[0], spacing)
                metrics["Case_ID"] = case_id
                all_metrics.append(metrics)
                
                # L·∫•y gi√° tr·ªã ƒë·ªÉ in
                d_wt = metrics.get('Dice_WT', 0)
                d_tc = metrics.get('Dice_TC', 0)
                d_et = metrics.get('Dice_ET', 0)
                d_mean = metrics.get('Mean_Dice', 0)
                
                # In d√≤ng k·∫øt qu·∫£ th·∫≥ng h√†ng
                print(f"{i+1:<8} | {case_id:<15} | {d_wt:.4f}   | {d_tc:.4f}   | {d_et:.4f}   | {d_mean:.4f}")
            else:
                print(f"{i+1:<8} | {case_id:<15} | {'Done (No Metrics)':<40}")

            if CONFIG["save_2d_snapshot"]:
                visualize_comparison(case_id, mri, gt, pred, unc_dict, CONFIG)
            
        except Exception as e:
            # In l·ªói nh∆∞ng kh√¥ng l√†m v·ª° layout b·∫£ng qu√° nhi·ªÅu
            print(f"\n‚ùå Error {case_id}: {e}")
            import traceback
            traceback.print_exc()

    # --- 3. T·ªîNG H·ª¢P B√ÅO C√ÅO ---
    if CONFIG["calc_metrics"] and all_metrics:
        df = pd.DataFrame(all_metrics)
        
        # S·∫Øp x·∫øp c·ªôt th√¥ng minh
        # ∆Øu ti√™n Case_ID, sau ƒë√≥ ƒë·∫øn c√°c ch·ªâ s·ªë Dice, r·ªìi HD95
        priority_cols = ["Case_ID", "Dice_WT", "Dice_TC", "Dice_ET", "Mean_Dice", 
                         "HD95_WT", "HD95_TC", "HD95_ET"]
        # Gi·ªØ l·∫°i c√°c c·ªôt kh√°c n·∫øu c√≥ (v√≠ d·ª• spacing...)
        final_cols = [c for c in priority_cols if c in df.columns] + [c for c in df.columns if c not in priority_cols]
        df = df[final_cols]
        
        # L∆∞u chi ti·∫øt
        csv_detail_name = CONFIG.get("file_csv_detail", "metrics_detailed.csv")
        detail_path = os.path.join(CONFIG["output_folder"], csv_detail_name)
        df.to_csv(detail_path, index=False)
        
        # T√≠nh trung b√¨nh v√† In b·∫£ng t·ªïng k·∫øt ƒë·∫πp
        if CONFIG["metrics_average"]:
            csv_summary_name = CONFIG.get("file_csv_summary", "metrics_summary.csv")
            summary_path = os.path.join(CONFIG["output_folder"], csv_summary_name)
            
            mean_df = df.drop(columns=["Case_ID"]).mean()
            mean_df.to_csv(summary_path)
            
            print("\n" + "="*60)
            print(f"{'üìä FINAL SUMMARY (AVERAGE)':^60}")
            print("-" * 60)
            print(f"{'Metric':<15} | {'WT':<10} | {'TC':<10} | {'ET':<10}")
            print("-" * 60)
            
            # L·∫•y gi√° tr·ªã an to√†n (tr√°nh l·ªói n·∫øu key thi·∫øu)
            m_d_wt = mean_df.get('Dice_WT', 0)
            m_d_tc = mean_df.get('Dice_TC', 0)
            m_d_et = mean_df.get('Dice_ET', 0)
            
            m_h_wt = mean_df.get('HD95_WT', 0)
            m_h_tc = mean_df.get('HD95_TC', 0)
            m_h_et = mean_df.get('HD95_ET', 0)

            print(f"{'Dice Score':<15} | {m_d_wt:.4f}     | {m_d_tc:.4f}     | {m_d_et:.4f}")
            print(f"{'HD95 (mm)':<15} | {m_h_wt:.4f}     | {m_h_tc:.4f}     | {m_h_et:.4f}")
            print("-" * 60)
            print(f"Overall Mean Dice: {mean_df.get('Mean_Dice', 0):.4f}")
            print(f"‚úÖ Report saved to: {CONFIG['output_folder']}")

    print("\n‚úÖ --- PIPELINE COMPLETED ---")

if __name__ == "__main__":
    main()